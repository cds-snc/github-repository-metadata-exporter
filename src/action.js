"use strict";

const core = require("@actions/core");
const github = require("@actions/github");
const { createAppAuth } = require("@octokit/auth-app");

const { postData, uploadToS3 } = require("./lib/forwarder.js");
const {
  queryActionDependencies,
  queryAllPRs,
  queryBranchProtection,
  queryCodeScanningAlerts,
  queryCodespaces,
  queryCommitCount,
  queryDependabotAlerts,
  queryRepository,
  queryRequiredFiles,
  queryRenovatePRs,
  queryUsers,
  queryWorkflows,
} = require("./lib/query.js");

const prefix = "GitHubMetadata_";
const chunkSize = 10;

const action = async () => {
  const logAnalyticsWorkspaceId = core.getInput("log-analytics-workspace-id");
  const logAnalyticsWorkspaceKey = core.getInput("log-analytics-workspace-key");

  const githubAppId = core.getInput("github-app-id");
  const githubAppInstallationId = core.getInput("github-app-installation-id");
  const githubAppPrivateKey = core.getInput("github-app-private-key");

  const orgDataRepo = core.getInput("org-data-repo");

  // S3 config from action parameters (set via secrets)
  const s3Bucket =
    core.getInput("s3-bucket") ||
    "cds-data-lake-raw-production/operations/github";
  const awsRegion = core.getInput("aws-region") || "ca-central-1";

  const auth = createAppAuth({
    appId: githubAppId,
    privateKey: githubAppPrivateKey,
  });

  const installationAuthentication = await auth({
    type: "installation",
    installationId: githubAppInstallationId,
  });

  const octokit = github.getOctokit(installationAuthentication.token);

  const owner = github.context.repo.owner;
  const repo = github.context.repo.repo;

  // Helper to send to S3
  async function sendToS3(data, type) {
    try {
      const key = `${type}/${owner}-${repo}-${new Date().toISOString()}.json`;
      await uploadToS3(s3Bucket, key, data, awsRegion);
      console.log(`‚úÖ Data sent to S3: ${key}`);
    } catch (error) {
      console.log(`‚ö†Ô∏è Failed to send ${type} data to S3: ${error.message}`);
      console.log("Skipping S3 upload and continuing workflow...");
    }
  }

  // Get repository data
  const repository = await queryRepository(octokit, owner, repo);
  await postData(
    logAnalyticsWorkspaceId,
    logAnalyticsWorkspaceKey,
    repository,
    prefix + "Repository"
  );
  console.log("‚úÖ Repository data sent to Azure Log Analytics");

  // Get all PRs modified today and write to S3 only
  try {
    const allPRs = await queryAllPRs(octokit, owner, repo);
    await sendToS3(allPRs, "AllPRs");
  } catch (error) {
    console.log(`‚ö†Ô∏è Failed to get AllPRs data: ${error.message}`);
    console.log("Skipping AllPRs data collection and continuing workflow...");
  }

  // Get all workflow runs from yesterday and send to S3
  try {
    const workflowsData = await queryWorkflows(octokit, owner, repo);
    await sendToS3(workflowsData, "Workflows");
  } catch (error) {
    console.log(`‚ö†Ô∏è Failed to get Workflows data: ${error.message}`);
    console.log(
      "Skipping Workflows data collection and continuing workflow..."
    );
  }

  // Get branch protection data for main branch
  const branchProtectionData = await queryBranchProtection(
    octokit,
    owner,
    repo,
    "main"
  );
  await postData(
    logAnalyticsWorkspaceId,
    logAnalyticsWorkspaceKey,
    branchProtectionData,
    prefix + "BranchProtection"
  );
  console.log("‚úÖ BranchProtection data sent to Azure Log Analytics");

  // Get commit count data
  const commitCountData = await queryCommitCount(octokit, owner, repo);
  await postData(
    logAnalyticsWorkspaceId,
    logAnalyticsWorkspaceKey,
    commitCountData,
    prefix + "CommitCount"
  );
  console.log("‚úÖ CommitCount data sent to Azure Log Analytics");
  await sendToS3(commitCountData, "CommitCount");

  // Get required files data for current branch
  const requiredFilesData = await queryRequiredFiles(owner, repo);
  await postData(
    logAnalyticsWorkspaceId,
    logAnalyticsWorkspaceKey,
    requiredFilesData,
    prefix + "RequiredFiles"
  );
  console.log("‚úÖ RequiredFiles data sent to Azure Log Analytics");

  // Get dependabot alerts data for current branch
  const dependabotAlertsData = await queryDependabotAlerts(
    octokit,
    owner,
    repo
  );
  await postData(
    logAnalyticsWorkspaceId,
    logAnalyticsWorkspaceKey,
    dependabotAlertsData,
    prefix + "DependabotAlerts"
  );
  console.log("‚úÖ DependabotAlerts data sent to Azure Log Analytics");
  await sendToS3(dependabotAlertsData, "DependabotAlerts");

  // Get code scanning alerts data for current branch
  const codeScanningAlertsData = await queryCodeScanningAlerts(
    octokit,
    owner,
    repo
  );

  // Breaks code scanning results into chunks of 10
  const codeScanningAlertsDataChunks =
    codeScanningAlertsData.code_scanning_alerts;

  for (let i = 0; i < codeScanningAlertsDataChunks.length; i += chunkSize) {
    const chunk = codeScanningAlertsDataChunks.slice(i, i + chunkSize);
    let data = {
      code_scanning_alerts: chunk,
    };

    await postData(
      logAnalyticsWorkspaceId,
      logAnalyticsWorkspaceKey,
      { ...codeScanningAlertsData, ...data },
      prefix + "CodeScanningAlerts"
    );
    console.log(
      `‚è±Ô∏è ${chunk.length} code scanning alerts sent to Azure Log Analytics.`
    );
  }
  console.log("‚úÖ CodeScanningAlerts data sent to Azure Log Analytics");

  // Get Renovate PRs data for current repo
  const renovatePRsData = await queryRenovatePRs(octokit, owner, repo);

  // Breaks code scanning results into chunks of 10
  const renovatePRsDataChunks = renovatePRsData.renovate_prs;

  for (let i = 0; i < renovatePRsDataChunks.length; i += chunkSize) {
    const chunk = renovatePRsDataChunks.slice(i, i + chunkSize);
    let data = {
      renovate_prs: chunk,
    };

    await postData(
      logAnalyticsWorkspaceId,
      logAnalyticsWorkspaceKey,
      { ...renovatePRsData, ...data },
      prefix + "RenovatePRs"
    );
    console.log(`‚è±Ô∏è ${chunk.length} renovate PRs sent to Azure Log Analytics.`);
  }
  console.log("‚úÖ RenovatePRs data sent to Azure Log Analytics");

  // Get required files data for current branch
  const actionDependenciesData = await queryActionDependencies(owner, repo);
  await postData(
    logAnalyticsWorkspaceId,
    logAnalyticsWorkspaceKey,
    actionDependenciesData,
    prefix + "ActionDependencies"
  );
  console.log("‚úÖ ActionDependencies data sent to Azure Log Analytics");

  // Get central repository data if current repo is org data repo
  if (orgDataRepo == `${owner}/${repo}`) {
    console.log("üêøÔ∏è Getting org data");

    // Get users data from org
    console.log("üë§ Getting user data");
    const usersData = await queryUsers(octokit, owner);

    const usersDataChunks = usersData.users;

    for (let i = 0; i < usersDataChunks.length; i += chunkSize) {
      const chunk = usersDataChunks.slice(i, i + chunkSize);
      let data = {
        users: chunk,
      };

      await postData(
        logAnalyticsWorkspaceId,
        logAnalyticsWorkspaceKey,
        { ...usersData, ...data },
        prefix + "Users"
      );
      console.log(`‚è±Ô∏è ${chunk.length} users sent to Azure Log Analytics.`);
    }
    console.log("‚úÖ Users data sent to Azure Log Analytics");

    // Get codespaces data from org
    console.log("üñ•Ô∏è Getting codespaces data");
    const codespacesData = await queryCodespaces(octokit, owner);

    const codespacesDataChunks = codespacesData.codespaces;

    for (let i = 0; i < codespacesDataChunks.length; i += chunkSize) {
      const chunk = codespacesDataChunks.slice(i, i + chunkSize);
      let data = {
        codespaces: chunk,
      };

      await postData(
        logAnalyticsWorkspaceId,
        logAnalyticsWorkspaceKey,
        { ...codespacesData, ...data },
        prefix + "Codespaces"
      );
      console.log(`‚è±Ô∏è ${chunk.length} codespaces sent to Azure Log Analytics.`);
    }
    console.log("‚úÖ Codespaces data sent to Azure Log Analytics");
  }
};

module.exports = {
  action: action,
};
